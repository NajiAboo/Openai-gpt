{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Video Summarization Steps\n",
        "### Convert frames to base64\n",
        "### Get audio content\n",
        "### Summarize the video only\n",
        "### summarize the audio only\n",
        "### summarize both audio and video together"
      ],
      "metadata": {
        "id": "Csv5zZKUeTA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade openai --quiet\n",
        "%pip install opencv-python --quiet\n",
        "%pip install moviepy --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1-H_LF8eQrq",
        "outputId": "08675287-d815-4d96-8d69-55cd298e3fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/320.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "open_ai_api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "O-ieAJnXe4mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "model = \"gpt-4o\"\n",
        "client = OpenAI(api_key= open_ai_api_key)"
      ],
      "metadata": {
        "id": "-Q1g003te4pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from moviepy.editor import VideoFileClip\n",
        "import time\n",
        "import base64"
      ],
      "metadata": {
        "id": "620SoAife4rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_PATH = \"/content/keynote_recap.mp4\""
      ],
      "metadata": {
        "id": "vV8Icinxhunv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, seconds_per_frame =2 ):\n",
        "  base64frames = []\n",
        "  base_video_path,_ = os.path.splitext(video_path)\n",
        "  print(base_video_path)\n",
        "\n",
        "  video = cv2.VideoCapture(video_path)\n",
        "  total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  fps = video.get(cv2.CAP_PROP_FPS)\n",
        "  frames_to_skip = int(fps*seconds_per_frame)\n",
        "\n",
        "  curr_frame = 0\n",
        "\n",
        "  while curr_frame < total_frames - 1:\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
        "    sucess, frame = video.read()\n",
        "\n",
        "    if not sucess:\n",
        "      break\n",
        "\n",
        "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
        "    base64frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
        "    curr_frame = curr_frame + frames_to_skip\n",
        "\n",
        "  video.release()\n",
        "\n",
        "  audio_path = f\"{base_video_path}.mp3\"\n",
        "  clip = VideoFileClip(video_path)\n",
        "  clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
        "  clip.audio.close()\n",
        "  clip.close()\n",
        "\n",
        "  return base64frames, audio_path"
      ],
      "metadata": {
        "id": "PYB2GFDnhup9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base64_frames, audio_path = process_video(VIDEO_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnI11aWJhusV",
        "outputId": "56ac5d60-bc2c-4e51-8054-1471013ef73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keynote_recap\n",
            "MoviePy - Writing audio in /content/keynote_recap.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base64_frames = base64_frames[::20]"
      ],
      "metadata": {
        "id": "D4PQ5XNre4tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "respose = client.chat.completions.create(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are generating a video summary, Please provide a summary of the video, repond in Markdown\"\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"user\",\n",
        "            \"content\": [\n",
        "                \"There are the frames from the video\",\n",
        "                *map(lambda x: {\"type\": \"image_url\",\n",
        "                                \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}\n",
        "\n",
        "                }, base64_frames)\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0,\n",
        ")"
      ],
      "metadata": {
        "id": "Z7g-7zJTe4wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(respose.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09EkBlhRe4y6",
        "outputId": "fa084b2f-600b-4c70-e5f4-e89cdd4e45a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Video Summary\n",
            "\n",
            "The video appears to be from an event called \"OpenAI DevDay.\" Here is a summary based on the provided frames:\n",
            "\n",
            "1. **Title Frame**: The video begins with a title screen displaying \"OpenAI DevDay.\"\n",
            "2. **Function Calling Slide**: A slide is shown explaining the concept of function calling, illustrating a \"before\" and \"after\" scenario where commands are transformed into function calls.\n",
            "3. **Speaker Presentation**: A speaker, dressed in a green sweater and dark pants, is presenting on stage. The background suggests a formal event setting.\n",
            "4. **Key Points Slide**: The speaker discusses three main points: Instructions, Expanded Knowledge, and Actions, as shown on a slide behind him.\n",
            "5. **Continued Presentation**: The speaker continues to elaborate on the topics, engaging with the audience.\n",
            "6. **Closing Gesture**: The speaker waves, possibly indicating the end of the presentation or a transition to another segment.\n",
            "\n",
            "The video likely covers advancements or features related to OpenAI's technology, focusing on practical applications and improvements in function calling and knowledge expansion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rU8BhsJHl9Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file = open(audio_path, \"rb\")\n",
        ")"
      ],
      "metadata": {
        "id": "Nwd0VIUFcgkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transcript:\", transcription.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2doopj8cgmR",
        "outputId": "14bee214-80f5-48dd-934c-6dd1390aef0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: Welcome to our first-ever OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. Dolly 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools, higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens, starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or if you're on ChatGPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are the GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = model ,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\" : \"system\",\n",
        "            \"content\": \" You are genrating a transcript summary, create  a summar of the provided transcription\"\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\" : \"text\",\n",
        "                    \"text\": f\"the audio tracription is : {transcription.text}\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "_XkHrE4Lcgoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7aAMWr7cguF",
        "outputId": "970e92b1-adb0-4607-8f17-48ed835d4912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the first-ever OpenAI Dev Day, several significant announcements were made:\n",
            "\n",
            "1. **GPT-4 Turbo**: A new model supporting up to 128,000 tokens of context, featuring JSON mode for valid JSON responses, improved instruction following, and better knowledge retrieval from external documents or databases. It is also more cost-effective than GPT-4.\n",
            "\n",
            "2. **New Features and Models**: \n",
            "   - **Dolly 3** and **GPT-4 Turbo with Vision**.\n",
            "   - **Text-to-Speech model**.\n",
            "   - **Custom Models** program for tailored solutions with higher rate limits and customizable quotas.\n",
            "\n",
            "3. **GPTs**: Tailored versions of ChatGPT for specific purposes, programmable through conversation, with options for private or public sharing, and a forthcoming GPT Store.\n",
            "\n",
            "4. **Assistance API**: Includes persistent threads, built-in retrieval, a code interpreter, and improved function calling.\n",
            "\n",
            "The event emphasized the potential of these technologies to provide \"superpowers on demand\" and hinted at even more advanced developments in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \" You are generating a video summary, create a summary of the provided video and its transcript. Reponse in Markdown\"\n",
        "\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                'These are the frame from the video',\n",
        "                 *map(lambda x: {\"type\": \"image_url\",\n",
        "                                \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}\n",
        "\n",
        "                }, base64_frames),\n",
        "\n",
        "                {\n",
        "                    \"type\" : \"text\",\n",
        "                    \"text\": f\"the audio tracription is : {transcription.text}\"\n",
        "                }\n",
        "\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0,\n",
        ")"
      ],
      "metadata": {
        "id": "Xp_eTR68dteW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C4eTlIffpCD",
        "outputId": "6e67d519-3eed-42e3-a314-6c5770eba93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Video Summary\n",
            "\n",
            "### Title: OpenAI Dev Day\n",
            "\n",
            "#### Key Highlights:\n",
            "\n",
            "1. **Introduction:**\n",
            "   - The event is titled \"OpenAI Dev Day.\"\n",
            "   - The speaker welcomes the audience to the first-ever OpenAI Dev Day.\n",
            "\n",
            "2. **Launch of GPT-4 Turbo:**\n",
            "   - A new model, GPT-4 Turbo, is introduced.\n",
            "   - GPT-4 Turbo supports up to 128,000 tokens of context.\n",
            "   - A new feature called JSON mode ensures the model responds with valid JSON.\n",
            "   - The model can call multiple functions simultaneously and follows instructions better.\n",
            "\n",
            "3. **Knowledge Retrieval:**\n",
            "   - A new retrieval feature allows bringing knowledge from external documents or databases into applications.\n",
            "   - GPT-4 Turbo has knowledge up to April 2023, with ongoing improvements.\n",
            "\n",
            "4. **New Models and Features:**\n",
            "   - Dolly 3, GPT-4 Turbo with Vision, and a new Text-to-Speech model are available in the API.\n",
            "   - A new program called Custom Models is launched, allowing companies to create custom models with the help of OpenAI researchers.\n",
            "   - Higher rate limits and doubled tokens per minute for established GPT-4 customers.\n",
            "   - GPT-4 Turbo is significantly cheaper than GPT-4.\n",
            "\n",
            "5. **Introduction of GPTs:**\n",
            "   - GPTs are tailored versions of ChatGPT for specific purposes.\n",
            "   - They combine instructions, expanded knowledge, and actions for better performance and control.\n",
            "   - Users can program GPTs through conversation, make private GPTs, and share them publicly or within a company.\n",
            "   - A GPT Store will be launched later in the month.\n",
            "\n",
            "6. **Assistance API:**\n",
            "   - The Assistance API includes persistent threads, built-in retrieval, a code interpreter, and improved function calling.\n",
            "\n",
            "7. **Closing Remarks:**\n",
            "   - The speaker expresses excitement about the future and the potential of the technology.\n",
            "   - An invitation is extended for the audience to return next year to see further advancements.\n",
            "\n",
            "### Visuals:\n",
            "- The frames show the speaker presenting on stage, slides with key points, and the audience.\n",
            "\n",
            "### Conclusion:\n",
            "The event showcases significant advancements in AI technology, particularly with the introduction of GPT-4 Turbo and various new features and models aimed at enhancing user experience and capabilities. The speaker emphasizes the potential of these technologies to empower users and invites the audience to look forward to future developments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Question: Why did Sam Altman have an example about raising windows and turning the radio on?\""
      ],
      "metadata": {
        "id": "Hr8C1e5Wfsih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use the video to answer the provied question, Respond in Markdown\"\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"user\",\n",
        "            \"content\": [\n",
        "                \"There are the frames from the video\",\n",
        "                *map(lambda x: {\"type\": \"image_url\",\n",
        "                                \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}\n",
        "\n",
        "                }, base64_frames),\n",
        "                QUESTION\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0,\n",
        ")"
      ],
      "metadata": {
        "id": "9hiGVScMgtvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ySmhutTNgtyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e8cc51-f905-4e44-9bca-94ac582ae22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sam Altman used the example about raising windows and turning the radio on to illustrate the concept of function calling. The example demonstrates how a command given in natural language (\"raise my windows and turn the radio on\") can be broken down into specific functions (raise_windows() and radio_on()) that a system can execute. This highlights the capability of advanced AI to understand and perform complex tasks by interpreting and acting on natural language instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = model ,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\" : \"system\",\n",
        "            \"content\": \" Use transcription to answer the provided question\"\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\" : \"text\",\n",
        "                    \"text\": f\"the audio tracription is : {transcription.text}. \\n\\n {QUESTION}\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "C0zwnxkziu80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM-IPTgxjSVg",
        "outputId": "af3d0afc-a699-4ed5-e706-f504bcc182a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided transcription does not include any mention of Sam Altman or an example about raising windows and turning the radio on. Therefore, I cannot provide an answer to that question based on the given transcription.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3VMEqqyjfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_winyEucjfFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use the video and transcription to anser the provided question\"\n",
        "\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                'These are the frame from the video',\n",
        "                 *map(lambda x: {\"type\": \"image_url\",\n",
        "                                \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}\n",
        "\n",
        "                }, base64_frames),\n",
        "\n",
        "                {\n",
        "                    \"type\" : \"text\",\n",
        "                    \"text\": f\"the audio tracription is : {transcription.text}\"\n",
        "                },\n",
        "\n",
        "                QUESTION\n",
        "\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0,\n",
        ")"
      ],
      "metadata": {
        "id": "YX1v3ONljfKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTR9yG12jx8H",
        "outputId": "5d0582bb-1d44-4c61-f784-6f13d3b9c330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sam Altman used the example about raising windows and turning the radio on to illustrate the improved function calling capabilities of the new model, GPT-4 Turbo. The example demonstrates how the model can now handle multiple function calls more effectively and follow instructions better. This improvement allows for more complex and accurate interactions, such as executing multiple commands in a single request.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPIyYmsfj08R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}